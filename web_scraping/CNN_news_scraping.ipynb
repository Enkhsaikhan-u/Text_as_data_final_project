{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b03d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5508d34",
   "metadata": {},
   "source": [
    "Scaring links using key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Chrome...\n",
      "Chrome launched successfully.\n",
      "Scraping keyword: inflation\n",
      "  New links found: 96\n",
      "Scraping keyword: stocks\n",
      "  New links found: 95\n",
      "Scraping keyword: economy\n",
      "  New links found: 86\n",
      "Scraping keyword: tax\n",
      "  New links found: 82\n",
      "Scraping keyword: taxes\n",
      "  New links found: 24\n",
      "Scraping keyword: business\n",
      "  New links found: 40\n",
      "Scraping keyword: finances\n",
      "  New links found: 85\n",
      "Scraping keyword: financial policy\n",
      "  New links found: 72\n",
      "Scraping keyword: economic policy\n",
      "  New links found: 43\n",
      "Scraping keyword: fiscal policy\n",
      "  New links found: 87\n",
      "Scraping keyword: GDP\n",
      "  New links found: 62\n",
      "Scraping keyword: unemployment\n",
      "  New links found: 82\n",
      "Scraping keyword: interest rates\n",
      "  New links found: 82\n",
      "Scraping keyword: recession\n",
      "  New links found: 48\n",
      "Scraping keyword: economic growth\n",
      "  New links found: 25\n",
      "Scraping keyword: budget deficit\n",
      "  New links found: 58\n",
      "Scraping keyword: trade deficit\n",
      "  New links found: 67\n",
      "Scraping keyword: consumer spending\n",
      "  New links found: 53\n",
      "Scraping keyword: investment\n",
      "  New links found: 69\n",
      "Scraping keyword: monetary policy\n",
      "  New links found: 37\n",
      "Scraping keyword: fiscal stimulus\n",
      "  New links found: 61\n",
      "Scraping keyword: housing market\n",
      "  New links found: 88\n",
      "Scraping keyword: labor market\n",
      "  New links found: 68\n",
      "Scraping keyword: wages\n",
      "  New links found: 95\n",
      "Scraping keyword: corporate earnings\n",
      "  New links found: 90\n",
      "Scraping keyword: supply chain\n",
      "  New links found: 64\n",
      "Scraping keyword: energy prices\n",
      "  New links found: 74\n",
      "Scraping keyword: commodity prices\n",
      "  New links found: 51\n",
      "Scraping keyword: financial markets\n",
      "  New links found: 45\n",
      "Scraping keyword: stock market volatility\n",
      "  New links found: 29\n",
      "Scraping keyword: economic outlook\n",
      "  New links found: 13\n",
      "Scraping keyword: economic indicators\n",
      "  New links found: 24\n",
      "Scraping keyword: central bank policy\n",
      "  New links found: 48\n",
      "Scraping keyword: inflation expectations\n",
      "  New links found: 14\n",
      "Scraping keyword: currency exchange rates\n",
      "  New links found: 73\n",
      "Scraping keyword: credit markets\n",
      "  New links found: 76\n",
      "Scraping keyword: business cycles\n",
      "  New links found: 26\n",
      "Scraping keyword: economic uncertainty\n",
      "  New links found: 22\n",
      "Scraping keyword: global economy\n",
      "  New links found: 19\n",
      "Scraping keyword: economic reforms\n",
      "  New links found: 41\n",
      "Scraping keyword: tax policy changes\n",
      "  New links found: 43\n",
      "----------------------------------------\n",
      "Scraping complete.\n",
      "Total new links saved: 2357\n",
      "File location: C:\\Users\\Enkhsaikhan\\Final_paper_text_as_data\\raw_data\\cnn_links.csv\n",
      "Preview: ['https://www.cnn.com/2025/12/23/economy/us-gdp-q3', 'https://www.cnn.com/2025/12/18/economy/us-cpi-consumer-price-index-november', 'https://www.cnn.com/2025/12/16/economy/affordability-wage-growth-inflation', 'https://www.cnn.com/2025/12/15/economy/affordability-jobs-inflation', 'https://www.cnn.com/2025/12/11/politics/inflation-trump-karoline-leavitt-prices']\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CNN ARTICLE LINK SCRAPER\n",
    "# ===============================\n",
    "# -------------------------------\n",
    "# -------------------------------\n",
    "# 1. Keywords\n",
    "# -------------------------------\n",
    "key_words = [\n",
    "    \"inflation\", \"stocks\", \"economy\", \"tax\", \"taxes\", \"business\", \"finances\", \"financial policy\", \"economic policy\",\n",
    "    \"fiscal policy\", \"GDP\", \"unemployment\", \"interest rates\", \"recession\", \"economic growth\", \"budget deficit\", \"trade deficit\",\n",
    "    \"consumer spending\", \"investment\", \"monetary policy\", \"fiscal stimulus\", \"housing market\", \"labor market\", \"wages\",\n",
    "    \"corporate earnings\", \"supply chain\", \"energy prices\", \"commodity prices\", \"financial markets\",\n",
    "    \"stock market volatility\", \"economic outlook\", \"economic indicators\", \"central bank policy\",\n",
    "    \"inflation expectations\", \"currency exchange rates\", \"credit markets\", \"business cycles\", \"economic uncertainty\", \"global economy\",\n",
    "    \"economic reforms\", \"tax policy changes\"\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Paths\n",
    "# -------------------------------\n",
    "RAW_DATA_DIR = r\"C:\\Users\\Enkhsaikhan\\Final_paper_text_as_data\\raw_data\"\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "\n",
    "CNN_LINKS_FILE = os.path.join(RAW_DATA_DIR, \"cnn_links.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load existing links (safe)\n",
    "# -------------------------------\n",
    "if os.path.exists(CNN_LINKS_FILE):\n",
    "    seen_links = set(\n",
    "        pd.read_csv(CNN_LINKS_FILE)[\"link\"].dropna().tolist()\n",
    "    )\n",
    "else:\n",
    "    seen_links = set()\n",
    "\n",
    "unique_urls_list = []\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Chrome setup\n",
    "# -------------------------------\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "service = Service()  # Selenium Manager handles chromedriver\n",
    "\n",
    "print(\"Launching Chrome...\")\n",
    "\n",
    "try:\n",
    "    driver = webdriver.Chrome(options=chrome_options, service=service)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    print(\"Chrome launched successfully.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 5. Scrape links\n",
    "    # -------------------------------\n",
    "    for key_word in key_words:\n",
    "        search_url = (\n",
    "            f\"https://edition.cnn.com/search\"\n",
    "            f\"?q={key_word}&from=0&size=100&page=1\"\n",
    "            f\"&sort=newest&types=article&section=\"\n",
    "        )\n",
    "\n",
    "        print(f\"Scraping keyword: {key_word}\")\n",
    "\n",
    "        try:\n",
    "            driver.get(search_url)\n",
    "        except Exception:\n",
    "            print(f\"  Timeout loading page for '{key_word}', continuing...\")\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.CLASS_NAME, \"container__headline-text\")\n",
    "                )\n",
    "            )\n",
    "        except Exception:\n",
    "            print(f\"  No results found for '{key_word}'\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        headlines = soup.find_all(\n",
    "            \"span\", class_=\"container__headline-text\"\n",
    "        )\n",
    "\n",
    "        new_count = 0\n",
    "\n",
    "        for h in headlines:\n",
    "            link = h.get(\"data-zjs-href\")\n",
    "\n",
    "            if not link:\n",
    "                parent = h.find_parent(\"a\")\n",
    "                if parent:\n",
    "                    link = parent.get(\"href\")\n",
    "\n",
    "            if not link:\n",
    "                continue\n",
    "\n",
    "            # Normalize CNN links\n",
    "            if link.startswith(\"/\"):\n",
    "                link = \"https://edition.cnn.com\" + link\n",
    "\n",
    "            if link not in seen_links:\n",
    "                seen_links.add(link)\n",
    "                unique_urls_list.append(link)\n",
    "                new_count += 1\n",
    "\n",
    "        print(f\"  New links found: {new_count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")\n",
    "\n",
    "finally:\n",
    "    if \"driver\" in locals():\n",
    "        driver.quit()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Save links\n",
    "# -------------------------------\n",
    "if unique_urls_list:\n",
    "    df_links = pd.DataFrame({\"link\": unique_urls_list})\n",
    "\n",
    "    df_links.to_csv(\n",
    "        CNN_LINKS_FILE,\n",
    "        mode=\"a\" if os.path.exists(CNN_LINKS_FILE) else \"w\",\n",
    "        header=not os.path.exists(CNN_LINKS_FILE),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"Scraping complete.\")\n",
    "print(f\"Total new links saved: {len(unique_urls_list)}\")\n",
    "print(f\"File location: {CNN_LINKS_FILE}\")\n",
    "print(\"Preview:\", unique_urls_list[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f69baa",
   "metadata": {},
   "source": [
    "Scraping whole articles using scraped links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72cc6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = r\"C:\\Users\\Enkhsaikhan\\Final_paper_text_as_data\\raw_data\"\n",
    "LINKS_FILE = os.path.join(RAW_DATA_DIR, \"cnn_links.csv\")\n",
    "OUTPUT_FILE = os.path.join(RAW_DATA_DIR, \"cnn_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64fcf5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2357 article links.\n"
     ]
    }
   ],
   "source": [
    "urls = (\n",
    "    pd.read_csv(LINKS_FILE)[\"link\"]\n",
    "    .dropna()\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(urls)} article links.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "471c4c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2357/2357 [2:36:24<00:00,  3.98s/it]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Scraping finished: 2272 articles saved.\n",
      "                                               title        date  \\\n",
      "0  The US economy expanded at the fastest pace in...  2025-12-23   \n",
      "1  Inflation cooled in November to 2.7%, but econ...  2025-12-18   \n",
      "2  The No. 1 cause of America’s affordability pro...  2025-12-16   \n",
      "3  The solution to America’s affordability proble...  2025-12-15   \n",
      "4  Analysis: How the White House is using mislead...  2025-12-11   \n",
      "\n",
      "                                                body  \\\n",
      "0  An initial reading of third-quarter gross dome...   \n",
      "1  Inflation unexpectedly – and sharply – slowed ...   \n",
      "2  America’scost-of-living problemis simple math:...   \n",
      "3  The best way to fix Americans’ cost-of-living ...   \n",
      "4  The year-over-yearinflation ratein January, th...   \n",
      "\n",
      "                                                link  \n",
      "0   https://www.cnn.com/2025/12/23/economy/us-gdp-q3  \n",
      "1  https://www.cnn.com/2025/12/18/economy/us-cpi-...  \n",
      "2  https://www.cnn.com/2025/12/16/economy/afforda...  \n",
      "3  https://www.cnn.com/2025/12/15/economy/afforda...  \n",
      "4  https://www.cnn.com/2025/12/11/politics/inflat...  \n"
     ]
    }
   ],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/121.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://edition.cnn.com/\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "def scrape_article(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "        # ---- Date from URL ----\n",
    "        m = re.search(r\"/(\\d{4})/(\\d{2})/(\\d{2})/\", url)\n",
    "        date = f\"{m.group(1)}-{m.group(2)}-{m.group(3)}\" if m else None\n",
    "\n",
    "        # ---- Title ----\n",
    "        title_tag = soup.find(\"h1\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "        # ---- Article body (multiple CNN layouts) ----\n",
    "        paragraphs = []\n",
    "\n",
    "        # Layout 1 (most common)\n",
    "        article = soup.find(\"article\")\n",
    "        if article:\n",
    "            paragraphs = article.find_all(\"p\")\n",
    "\n",
    "        # Layout 2\n",
    "        if not paragraphs:\n",
    "            body = soup.find(\"div\", {\"data-component-name\": \"article-body\"})\n",
    "            if body:\n",
    "                paragraphs = body.find_all(\"p\")\n",
    "\n",
    "        # Layout 3 (legacy)\n",
    "        if not paragraphs:\n",
    "            body = soup.find(\"div\", class_=\"article__content\")\n",
    "            if body:\n",
    "                paragraphs = body.find_all(\"p\")\n",
    "\n",
    "        text = \"\\n\".join(\n",
    "            p.get_text(strip=True)\n",
    "            for p in paragraphs\n",
    "            if p.get_text(strip=True)\n",
    "        )\n",
    "\n",
    "        if not text:\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"date\": date,\n",
    "            \"body\": text,\n",
    "            \"link\": url\n",
    "        }\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "results = []\n",
    "\n",
    "print(\"Scraping articles...\")\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    article = scrape_article(url)\n",
    "    if article:\n",
    "        results.append(article)\n",
    "\n",
    "    # Random delay to avoid detection\n",
    "    time.sleep(random.uniform(0.5, 1.2))\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Scraping finished: {len(df)} articles saved.\")\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
